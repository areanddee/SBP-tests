{\rtf1\ansi\ansicpg1252\cocoartf2867
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\froman\fcharset0 Times-Roman;\f1\froman\fcharset0 Times-Bold;\f2\fmodern\fcharset0 Courier;
}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid2}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sa240\partightenfactor0

\f0\fs24 \cf0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Great questions, both worth addressing.\
\pard\pardeftab720\sa240\partightenfactor0

\f1\b \cf0 \strokec2 1. Projection efficiency:
\f0\b0 \strokec2  Yes, it's wasteful right now. 
\f2\fs26 \strokec2 A
\f0\fs24 \strokec2  is identity except at rows 0 and N \'97 it just averages two boundary values. The full matrix multiply is O(N\'b2) per axis when O(N) suffices:\
\pard\pardeftab720\partightenfactor0

\f2\fs26 \cf0 \strokec2 def project_h(h, A):\
    # What we do now: A @ h @ A  \uc0\u8594   O(N\'b3) total\
    # What we should do:\
    avg = 0.5 * (h[0] + h[-1])  # for equal Hv weights\
    h = h.at[0].set(avg)\
    h = h.at[-1].set(avg)\
    # then same along axis 1\
\pard\pardeftab720\sa240\partightenfactor0

\f0\fs24 \cf0 \strokec2 For the single-panel periodic case it's trivial. On the cubed sphere it becomes averaging at the 12 panel edges \'97 still sparse. For now the matrix form is fine for correctness validation, but when we go to GPU performance we'll absolutely want the sparse version. Good instinct.\
\pard\pardeftab720\sa240\partightenfactor0

\f1\b \cf0 \strokec2 2. Energy-conserving learned integrator:
\f0\b0 \strokec2  This is a genuinely interesting idea and yes, this is a uniquely good setting for it. Here's why:\
The SBP spatial discretization gives you something most ML-for-physics work doesn't have: a 
\f1\b \strokec2 provably energy-conserving semi-discrete system
\f0\b0 \strokec2 . You know the ODE 
\f2\fs26 \strokec2 dy/dt = f(y)
\f0\fs24 \strokec2  conserves energy exactly. The only energy error comes from time-stepping. So you have:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls1\ilvl0
\f1\b \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Clean training signal
\f0\b0 : Run RK4 at very small dt (CFL=0.025, dt\uc0\u8309  energy error ~10\u8315 \'b9\'b9) to generate near-exact trajectories\
\ls1\ilvl0
\f1\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Known conserved quantity
\f0\b0 : E = (g/2)\uc0\u8747 h\'b2 + (H\u8320 /2)\u8747 v\'b2 with the SBP quadrature weights\
\ls1\ilvl0
\f1\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Measurable improvement target
\f0\b0 : Can the learned integrator take CFL=0.3 steps with energy error better than RK4's 10\uc0\u8315 \u8309 ?\
\pard\pardeftab720\sa240\partightenfactor0
\cf0 \strokec2 The literature connection is Hamiltonian Neural Networks (Greydanus et al. 2019) and symplectic integrators, but this would be different \'97 you're not learning the Hamiltonian, you're learning a 
\f1\b \strokec2 time integrator for a known energy-conserving system
\f0\b0 \strokec2 . Possible architectures:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls2\ilvl0\cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Learn a correction to RK4: 
\f2\fs26 y_\{n+1\} = RK4(y_n, dt) + NN(y_n, dt)
\f0\fs24  with energy penalty\
\ls2\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Learn in the symplectic framework: parameterize the integrator as a composition of energy-preserving maps\
\ls2\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Or simply: train a step function with a hard constraint that 
\f2\fs26 E(y_\{n+1\}) = E(y_n)
\f0\fs24  via projection onto the energy manifold\
\pard\pardeftab720\sa240\partightenfactor0
\cf0 \strokec2 The publishable angle: 
\f1\b \strokec2 "SBP-guaranteed spatial conservation + learned temporal conservation = machine-precision energy conservation at operational CFL."
\f0\b0 \strokec2  That's a result nobody has because nobody else starts with provably conservative spatial operators.\
Want me to sketch this as a short research note, or shall we stay focused on getting through Steps 3\'965 first and circle back?\
}